Dataflow concurrency offers an alternative concurrency model, which is inherently safe and robust.

h2. Introduction

Check out the small example written in Groovy using GPars, which sums results of calculations performed by three concurrently run tasks:
{code}
import static groovyx.gpars.dataflow.DataFlow.task

final def x = new DataFlowVariable()
final def y = new DataFlowVariable()
final def z = new DataFlowVariable()

task {
    z << x.val + y.val
}

task {
    x << 10
}

task {
    y << 5
}

println "Result: ${z.val}"
{code}

Or the same algorithm rewritten using the _DataFlows_ class.

{code}

import static groovyx.gpars.dataflow.DataFlow.task

final def df = new DataFlows()

task {
    df.z = df.x + df.y
}

task {
    df.x = 10
}

task {
    df.y = 5
}

println "Result: ${df.z}"

{code}

We start three logical tasks, which can run in parallel and perform their particular activities. The tasks need to exchange data and they do so using *Dataflow Variables*.
Think of Dataflow Variables as one-shot channels safely and reliably transferring data from producers to their consumers.

The Dataflow Variables have a pretty straightforward semantics. When a task needs to read a value from _DataFlowVariable_ (through the val property), it will block until the value has been set by another task or thread (using the '<<' operator). Each _DataFlowVariable_ can be set *only once* in its lifetime. Notice that you don't have to bother with ordering and synchronizing the tasks or threads and their access to shared variables. The values are magically transferred among tasks at the right time without your intervention.
The data flow seamlessly among tasks / threads without your intervention or care.

*Implementation detail:* The three tasks in the example *do not necessarily need to be mapped to three physical threads*. Tasks represent so-called "green" or "logical" threads and can be mapped under the covers to any number of physical threads. The actual mapping depends on the scheduler, but the outcome of dataflow algorithms doesn't depend on the actual scheduling.

{note}
The _bind_ operation of dataflow variables silently accepts re-binding to a value, which is equal to an already bound value. Call _bindUnique_ to reject equal values on already-bound variables.
{note}

h2. Benefits

Here's what you gain by using Dataflow Concurrency (by "Jonas BonÃ©r":http://www.jonasboner.com ):

* No race-conditions
* No live-locks
* Deterministic deadlocks
* Completely deterministic programs
* BEAUTIFUL code.

This doesn't sound bad, does it?

h1. Concepts

h2. Dataflow programming

h4. Quoting Wikipedia

Operations (in Dataflow programs) consist of "black boxes" with inputs and outputs, all of which are always explicitly defined. They run as soon as all of their inputs become valid, as opposed to when the program encounters them. Whereas a traditional program essentially consists of a series of statements saying "do this, now do this", a dataflow program is more like a series of workers on an assembly line, who will do their assigned task as soon as the materials arrive. This is why dataflow languages are inherently parallel; the operations have no hidden state to keep track of, and the operations are all "ready" at the same time.

h2. Principles

With Dataflow Concurrency you can safely share variables across tasks. These variables (in Groovy instances of the _DataFlowVariable_ class) can only be assigned (using the '<<' operator) a value once in their lifetime. The values of the variables, on the other hand, can be read multiple times (in Groovy through the val property), even before the value has been assigned. In such cases the reading task is suspended until the value is set by another task.
So you can simply write your code for each task sequentially using Dataflow Variables and the underlying mechanics will make sure you get all the values you need in a thread-safe manner.

In brief, you generally perform three operations with Dataflow variables:
* Create a dataflow variable
* Wait for the variable to be bound (read it)
* Bind the variable (write to it)

And these are the three essential rules your programs have to follow:
* When the program encounters an unbound variable it waits for a value.
* It is not possible to change the value of a dataflow variable once it is bound.
* Dataflow variables makes it easy to create concurrent stream agents.

h2. Dataflow Streams and Queues

Before you go to check the samples of using *Dataflow Variables*, *Tasks* and *Operators*, you should know a bit about streams and queues to have a full picture of Dataflow Concurrency.
Except for dataflow variables there are also the concepts of _DataFlowQueues_ and _DataFlowStream_ that you can leverage in your code.
You may think of them as thread-safe buffers or queues. Check out a typical producer-consumer demo:

{code}import static groovyx.gpars.dataflow.DataFlow.task

def words = ['Groovy', 'fantastic', 'concurrency', 'fun', 'enjoy', 'safe', 'GPars', 'data', 'flow']
final def buffer = new DataFlowQueue()

task {
    for (word in words) {
        buffer << word.toUpperCase()  //add to the buffer
    }
}

task {
    while(true) println buffer.val  //read from the buffer in a loop
}
{code}

Both _DataFlowVariables_ and _DataFlowQueues_ implement the _DataFlowChannel_ interface with common methods allowing users
to read values from them. The ability to treat both types identically through the _DataFlowChannel_ interface comes in handy
once you start using them to wire _tasks_, _operators_ or _selectors_ together.

{note}
The _DataFlowChannel_ interface combines two interfaces, each serving its purpose:
* _DataFlowReadChannel_ holding all the methods necessary for reading values from a channel
* _DataFlowWriteChannel_ holding all the methods necessary for writing values into a channel
You may prefer using these dedicated interfaces instead of the general _DataFlowChannel_ interface, to better express the intended usage.
{note}

The _DataFlowChannel_ class, unlike the other communication elements, does not implement the _DataFlowChannel_ interface, since the semantics of its use is different.
(We'll talk more on _DataFlowStream_ purpose later.)
Use _DataFlowStreamReadAdapter_ and _DataFlowStreamWriteAdapter_ classes to wrap instances of the _DataFlowChannel_ class
in _DataFlowReadChannel_ or _DataFlowWriteChannel_ implementations.

h2. DataFlowStream


h3. DataFlowStream Adapters

Since the _DataFlowStream_ API as well as the semantics of its use are very different from the one defined by _DataFlow(Read/Write)Channel_ , adapters have to be used in order to allow _DataFlowStreams_
to be used with other dataflow elements.
The _DataFlowStreamReadAdapter_ class will wrap a _DataFlowStream_ with necessary methods to read values, while the _DataFlowStreamWriteAdapter_ class
will provide write methods around the wrapped _DataFlowStream_ .

{note}
It is important to mention that the _DataFlowStreamWriteAdapter_ is thread safe allowing multiple threads to add values to the wrapped _DataFlowStream_ through the adapter.
On the other hand, _DataFlowStreamReadAdapter is designed to be used by a single thread.
To minimize the overhead and stay in-line with the DataFlowStream semantics, the _DataFlowStreamReadAdapter_ class is not thread-safe
and should only be used from within a single thread.
If multiple threads need to read from a DataFlowStream, they should each create their own wrapping _DataFlowStreamReadAdapter_ .
{note}

Thanks to the adapters _DataFlowStream_ can be used for communication between operators or selectors, which expect _DataFlow(Read/Write)Channels_ .

{code}
import groovyx.gpars.dataflow.DataFlowQueue
import groovyx.gpars.dataflow.stream.DataFlowStream
import groovyx.gpars.dataflow.stream.DataFlowStreamReadAdapter
import groovyx.gpars.dataflow.stream.DataFlowStreamWriteAdapter
import static groovyx.gpars.dataflow.DataFlow.selector
import static groovyx.gpars.dataflow.DataFlow.operator

/**
 * Demonstrates the use of DataFlowStreamAdapters to allow dataflow operators to use DataFlowStreams
 */

final DataFlowStream a = new DataFlowStream()
final DataFlowStream b = new DataFlowStream()
def aw = new DataFlowStreamWriteAdapter(a)
def bw = new DataFlowStreamWriteAdapter(b)
def ar = new DataFlowStreamReadAdapter(a)
def br = new DataFlowStreamReadAdapter(b)

def result = new DataFlowQueue()

def op1 = operator(ar, bw) {
    bindOutput it
}
def op2 = selector([br], [result]) {
    result << it
}

aw << 1
aw << 2
aw << 3
assert([1, 2, 3] == [result.val, result.val, result.val])
op1.stop()
op2.stop()
op1.join()
op2.join()

{code}

Also the ability to select a value from multiple _DataFlowChannels_ can only be used through an adapter around a _DataFlowStream_ :

{code}
import groovyx.gpars.dataflow.Select
import groovyx.gpars.dataflow.stream.DataFlowStream
import groovyx.gpars.dataflow.stream.DataFlowStreamReadAdapter
import groovyx.gpars.dataflow.stream.DataFlowStreamWriteAdapter
import static groovyx.gpars.dataflow.DataFlow.select
import static groovyx.gpars.dataflow.DataFlow.task

/**
 * Demonstrates the use of DataFlowStreamAdapters to allow dataflow select to select on DataFlowStreams
 */

final DataFlowStream a = new DataFlowStream()
final DataFlowStream b = new DataFlowStream()
def aw = new DataFlowStreamWriteAdapter(a)
def bw = new DataFlowStreamWriteAdapter(b)
def ar = new DataFlowStreamReadAdapter(a)
def br = new DataFlowStreamReadAdapter(b)

final Select<?> select = select(ar, br)
task {
    aw << 1
    aw << 2
    aw << 3
}
assert 1 == select().value
assert 2 == select().value
assert 3 == select().value
task {
    bw << 4
    aw << 5
    bw << 6
}
def result = (1..3).collect{select()}.sort{it.value}
assert result*.value == [4, 5, 6]
assert result*.index == [1, 0, 1]
{code}
h2. Bind handlers

{code}
def a = new DataFlowVariable()
a >> {println "The variable has just been bound to $it"}
a.whenBound {println "Just to confirm that the variable has been really set to $it"}
...
{code}

A bound handlers can be registered on all dataflow channels (variables, queues or streams) either using the >> operator or the _whenBound()_ method. They will be run once a value is bound to the variable.

Dataflow queues also support a _wheneverBound_ method to register a closure or a message handler to run each time a value is bound to them.

{code}
def stream = new DataFlowQueue()
stream.wheneverBound {println "A value $it arrived to the stream"}
{code}

{note}
Dataflow variables and streams are one of several possible ways to implement _Parallel Speculations_ . For details, please check out _Parallel Speculations_ in the _Parallel Collections_ section
of the User Guide.
{note}

h2. Further reading

"Scala Dataflow library":http://github.com/jboner/scala-dataflow/tree/f9a38992f5abed4df0b12f6a5293f703aa04dc33/src by Jonas BonÃ©r

"JVM concurrency presentation slides":http://jonasboner.com/talks/state_youre_doing_it_wrong/html/all.html by Jonas BonÃ©r

"Dataflow Concurrency library for Ruby":http://github.com/larrytheliquid/dataflow/tree/master
