Dataflow Operators and Selectors provide a full Dataflow implementation with all the usual ceremony.

h3. Concepts

Full dataflow concurrency builds on the concept of channels connecting operators and selectors, which consume
values coming through input channels, transform them into new values and output the new values into their output channels.
While _Operators_ wait for *all* input channels to have a value available for read before they start process them,
_Selectors_ are triggered by a value available on *any* of the input channels.

{code}
operator(inputs: [a, b, c], outputs: [d]) {x, y, z ->
    ...
    bindOutput 0, x + y + z
}
{code}

{code}
/**
 * CACHE
 *
 * Caches sites' contents. Accepts requests for url content, outputs the content. Outputs requests for download
 * if the site is not in cache yet.
 */
operator(inputs: [urlRequests], outputs: [downloadRequests, sites]) {request ->

    if (!request.content) {
        println "[Cache] Retrieving ${request.site}"
        def content = cache[request.site]
        if (content) {
            println "[Cache] Found in cache"
            bindOutput 1, [site: request.site, word:request.word, content: content]
        } else {
            def downloads = pendingDownloads[request.site]
            if (downloads != null) {
                println "[Cache] Awaiting download"
                downloads << request
            } else {
                pendingDownloads[request.site] = []
                println "[Cache] Asking for download"
                bindOutput 0, request
            }
        }
    } else {
        println "[Cache] Caching ${request.site}"
        cache[request.site] = request.content
        bindOutput 1, request
        def downloads = pendingDownloads[request.site]
        if (downloads != null) {
            for (downloadRequest in downloads) {
                println "[Cache] Waking up"
                bindOutput 1, [site: downloadRequest.site, word:downloadRequest.word, content: request.content]
            }
            pendingDownloads.remove(request.site)
        }
    }
}
{code}

The standard error handling will print out an error message to standard error output and stop the operator in case an uncaught
exception is thrown from withing the operator's body. To alter the behavior, you can redefine the _reportError()_ method
on the operator:

{code}
    op.metaClass.reportError = {Throwable e ->
        //handle the exception
        stop()  //You can also stop the operator
    }
{code}

h4. Types of operators

There are specialized versions of operators serving specific purposes:

  * operator - the basic general-purpose operator
  * selector - operator that is triggered by a value being available in any of its input channels
  * prioritySelect - a selector that prefers delivering messages from lower-indexed input channels over higher-indexed ones
  * splitter - a single-input operator copying its input values to all of its output channels

h4. Chaining operators

Operators are typically combined into networks, when some operators consume output by other operators.

{code}
operator(inputs:[a, b], outputs:[c, d]) {...}
splitter(c, [e, f])
selector(inputs:[e, d]: outputs:[]) {...}
{code}

You may alternatively refer to output channels through operators:

{code}
def op1 = operator(inputs:[a, b], outputs:[c, d]) {...}
def sp1 = splitter(op1.outputs[0], [e, f])                            //takes the first output of op1
selector(inputs:[sp1.outputs[0], op1.outputs[1]]: outputs:[]) {...}   //takes the first output of sp1 and the second output of op1
{code}

h3. Parallelize operators

By default an operator's body is processed by a single thread at a time. While this is a safe setting allowing the operator's
body to be written in a non-thread-safe manner, once an operator becomes "hot" and data start to accumulate in the operator's
input queues, you might consider allowing multiple threads to run the operator's body concurrently. Bear in mind that in such a case
you need to avoid or protect shared resources from multi-threaded access.
To enable multiple threads to run the operator's body concurrently, pass an extra _maxForks_ parameter when creating an operator:

{code}
    def op = operator(inputs: [a, b, c], outputs: [d, e], maxForks: 2) {x, y, z ->
        bindOutput 0, x + y + z
        bindOutput 1, x * y * z
    }
{code}

The value of the _maxForks_ parameter indicates the maximum of threads running the operator concurrently. Only positive
numbers are allowed with value 1 being the default.

h4. Synchronizing the output

When enabling internal parallelization of an operator by setting the value for _maxForks_ to a value greater than 1
it is important to remember that without explicit or implicit synchronization in the operators' body race-conditions may occur.
Especially bear in mind that values written to multiple output channels are not guarantied to be written atomically in the same order to all the channels
 {code}
    operator(inputs:[inputChannel], outputs:[a, b], maxForks:5) {msg ->
        bindOutput 0, msg
        bindOutput 1, msg
    }
    inputChannel << 1
    inputChannel << 2
    inputChannel << 3
    inputChannel << 4
    inputChannel << 5
 {code}
 May result output streams having the values mixed-up something like:
 a -> 1, 3, 2, 4, 5
 b -> 2, 1, 3, 5, 4

 Explicit synchronization is one way to get correctly bound all output channels and protect operator not-thread local state:
 {code}
    def lock = new Object()
    operator(inputs:[inputChannel], outputs:[a, b], maxForks:5) {msg ->
        doStuffThatIsThreadSafe()

        synchronized(lock) {
            doSomethingThatMustNotBeAccessedByMultipleThreadsAtTheSameTime()
            bindOutput 0, msg
            bindOutput 1, 2*msg
        }
    }
 {code}

Obviously you need to weight the pros and cons here, since synchronization defeats the purpose of setting _maxForks_ to a value greater than 1.

To set values to all of the output channels, you may also consider calling either the _bindAllOutputsAtomically_ method, passing in
a single value to write to all output channels or the _bindAllOutputsAtomically_ method, which takes a list of values, each of which will be written
to the output channel with the same position index.

{note}
 Using the _bindAllOutputs_ method will not guarantee atomicity of writes across al the output channels when using internal parallelism.
 If preserving the order of messages in multiple output streams is not an issue, _bindAllOutputs_ will provide better performance over the atomic variants.
{note}
 
h3. Grouping operators

Dataflow operators can be organized into groups to allow for performance fine-tuning. Groups provide a handy _operator()_ factory method
to create tasks attached to the groups.

{code}
import groovyx.gpars.group.DefaultPGroup

def group = new DefaultPGroup()

group.with {
    operator(inputs: [a, b, c], outputs: [d]) {x, y, z ->
        ...
        bindOutput 0, x + y + z
    }
}
{code}

{note:Title=Custom thread pools for dataflow}
The default thread pool for dataflow operators contains non-daemon threads, which means your application will not exit before all operators are stopped.
When grouping operators, make sure that your custom thread pools either use daemon threads, too, which can be achieved by
using DefaultPGroup or by providing your own thread factory to a thread pool constructor,
or in case your thread pools use non-daemon threads, such as when using the NonDaemonPGroup group class, make sure you shutdown the group or the thread pool explicitly by calling its shutdown() method,
otherwise your applications will not exit.
{note}

h2. Selectors

Selector's body should be a closure consuming either one or two arguments.
{code}
selector ([inputs : [a, b, c], outputs : [d, e]) {value ->
    ....
}
{code}

The two-argument closure will get a value plus an index of the input channel, the value of which is currently being processed.
This allows the selector to distinguish between values coming through different input channels.

{code}
selector ([inputs : [a, b, c], outputs : [d, e]) {value, index ->
    ....
}
{code}

h3. Join selector

A selector without a body closure specified will copy incoming values to all output channels.

{code}
def join = selector ([inputs : [programmers, analysis, managers], outputs : [employees, colleagues])
{code}

h3. Internal parallelism

The _maxForks_ attribute allowing for internal selectors parallelism is also available.

{code}
selector ([inputs : [a, b, c], outputs : [d, e], maxForks : 5) {value ->
    ....
}
{code}

h2. Selecting a value from multiple channels

Frequently a value needs to be obtained from one of several dataflow channels (variables or streams). The _Select_ construct
is suitable for such scenarios.

{code}
import groovyx.gpars.dataflow.DataFlowVariable
import static groovyx.gpars.dataflow.DataFlow.select
import static groovyx.gpars.dataflow.DataFlow.task
import groovyx.gpars.dataflow.DataFlowStream

/**
* Shows a basic use of Select, which monitors a set of input channels for values and makes these values
* available on its output irrespective of their original input channel.
* Note that dataflow variables and streams can be combined for Select.
*
* You might also consider checking out the PrioritySelect class, which prioritizes values by the index of their input channel
*/
def a = new DataFlowVariable()
def b = new DataFlowVariable()
def c = new DataFlowStream()

task {
  sleep 3000
  a << 10
}

task {
  sleep 1000
  b << 20
}

task {
  sleep 5000
  c << 30
}

def select = select(a, b, c)
println "The fastest result is ${select.val}"
{code}

There are multiple ways to read values from a Select, leveraging the implemented _DataFlowChannel_ interface or the contained _outputChannel_ property:

{code}
def value = select.val                              //Compatible with other dataflow channels (variables or streams)
def value = select.getVal(10, TimeUnit.SECONDS)     //Compatible with other dataflow channels (variables or streams)
def value = select.getValAsync(actor)               //Compatible with other dataflow channels (variables or streams)
def value = select.getValAsync(requestId, actor)    //Compatible with other dataflow channels (variables or streams)
def value = select()                                //Calling the default call() method on Select
def value = select.outputChannel.val                //Using the output dataflow channel of the Select directly
def value = select.outputChannel.getValAsync(...)   //The Select output is a good old dataflow stream with all its handy methods for both synchronous and asynchronous value retrieval
{code}

h3. Priority Select

When certain channels should have precedence over others used by Select, the PrioritySelect construct should be used instead.

{code}
/**
 * Shows a basic use of PrioritySelect, which monitors a set of input channels for values and makes these values
 * available on its output irrespective of their original input channel.
 * Note that dataflow variables and streams can be combined for Select.
 * Unlike plain Select, the PrioritySelect class gives precedence to input channels with lower index.
 * Available messages from high priority channels will be served before messages from lower-priority channels.
 * Messages received through a single input channel will have their mutual order preserved.
 *
 */
def critical = new DataFlowVariable()
def ordinary = new DataFlowStream()
def whoCares = new DataFlowStream()

task {
    ordinary << 'All working fine'
    whoCares << 'I feel a bit tired'
    ordinary << 'We are on target'
}

task {
    ordinary << 'I have just started work. Will come back later...'
    sleep 5000
    ordinary << 'I am done for now'
}

task {
    whoCares << 'Huh, what is that noise'
    ordinary << 'Here I am to do some clean-up work'
    whoCares << 'I wonder whether unplugging this cable will eliminate that nasty sound.'
    critical << 'The server room goes on UPS!'
    whoCares << 'The sound has disappeared'
}

def select = prioritySelect(critical, ordinary, whoCares)
println 'Starting to monitor our IT department'
sleep 3000
10.times {println "Received: ${select.val}"}
{code}

Since both _Select_ and _PrioritySelect_ make their output channels accessible through the _outputChannel_ property,
they can be combined into dataflow networks with dataflow operators and selectors, which opens the door to many interesting implementations.
